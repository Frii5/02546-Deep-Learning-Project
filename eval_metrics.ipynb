{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd02747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import gzip\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17cf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on device: cpu\n",
      "#test examples: 10000, total chars: 7059156\n"
     ]
    }
   ],
   "source": [
    "# 0. DEVICE\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Evaluating on device:\", device)\n",
    "\n",
    "# 1. TOKENIZER / MODEL NAMES\n",
    "tokenizer_names = [\n",
    "    \"gpt2\",                 # 2. Byte-level BPE\n",
    "    \"google/byt5-small\",    # 3. Character-level\n",
    "    \"unigram\",              # 4. Your custom unigram\n",
    "]\n",
    "\n",
    "# 2. LOAD RAW VALIDATION DATA ONCE\n",
    "raw_eval = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:20%]\")\n",
    "raw_eval = raw_eval.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "eval_texts = list(raw_eval[\"text\"])\n",
    "total_chars = sum(len(t) for t in eval_texts)\n",
    "print(f\"#test examples: {len(eval_texts)}, total chars: {total_chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38dabd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating tokenizer: gpt2\n",
      "Computing true character usage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 7596.00 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating tokenizer: google/byt5-small\n",
      "Computing true character usage...\n",
      "\n",
      " Evaluating tokenizer: unigram\n",
      "Computing true character usage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 7435.04 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary table ===\n",
      "           tokenizer  vocab_size  params_millions  tokens_per_char  val_loss  \\\n",
      "0               gpt2       50258            81.91             0.24      8.77   \n",
      "1  google/byt5-small         384            43.61             1.00      2.37   \n",
      "2            unigram       32000            67.89             0.26      6.35   \n",
      "\n",
      "   perplexity  bits_per_char  \n",
      "0     6407.41           1.08  \n",
      "1       10.69           0.55  \n",
      "2      572.41           0.75  \n"
     ]
    }
   ],
   "source": [
    "# 3. EVALUATION FUNCTION FOR ONE MODEL\n",
    "def evaluate_model(name: str) -> Dict[str, Any]:\n",
    "    print(f\"\\n Evaluating tokenizer: {name}\")\n",
    "\n",
    "    # 3.1 Load tokenizer & model from disk\n",
    "    model_dir = os.path.join(\"model\", name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_dir).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    vocab_size = len(tokenizer)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # 3.2 TRUE CHARACTER COUNT FROM TOKENIZATION\n",
    "    print(\"Computing true character usage...\")\n",
    "    free_enc = tokenizer(\n",
    "        eval_texts,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "    chars_from_tokens = 0\n",
    "    total_tokens_in_free_enc = 0\n",
    "\n",
    "    for ids in free_enc[\"input_ids\"]:\n",
    "        decoded = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        chars_from_tokens += len(decoded)\n",
    "        total_tokens_in_free_enc += len(ids)\n",
    "\n",
    "    tokens_per_char = total_tokens_in_free_enc / chars_from_tokens\n",
    "    \n",
    "    # 3.4 PREPARE PADDED DATASET FOR LOSS / PERPLEXITY\n",
    "    def encode(batch):\n",
    "        texts = list(batch[\"text\"])\n",
    "        return tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=64,\n",
    "        )\n",
    "\n",
    "    eval_ds = raw_eval.map(encode, batched=True)\n",
    "    eval_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    eval_loader = DataLoader(eval_ds, batch_size=8)\n",
    "    \n",
    "    # 3.5 COMPUTE TOTAL NLL, LOSS, PERPLEXITY\n",
    "    total_nll = 0.0            # sum of nats over all non-pad tokens\n",
    "    total_nonpad_tokens = 0\n",
    "    total_forward_tokens = 0        # tokens processed (non-pad)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            labels = inputs.clone()\n",
    "            labels[attention_mask == 0] = -100\n",
    "\n",
    "            nonpad = (labels != -100).sum().item()\n",
    "            if nonpad == 0:\n",
    "                continue\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=inputs,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "            # loss accounting\n",
    "            loss = outputs.loss.item()\n",
    "            batch_nll = loss * nonpad\n",
    "            total_nll += batch_nll\n",
    "            total_nonpad_tokens += nonpad\n",
    "\n",
    "        avg_loss = total_nll / total_nonpad_tokens\n",
    "        ppl = math.exp(avg_loss)\n",
    "        \n",
    "        # 3.6 BITS-PER-TOKEN & BITS-PER-CHAR\n",
    "        total_bits = total_nll / math.log(2)\n",
    "        bits_per_char = total_bits / chars_from_tokens\n",
    "        \n",
    "        # 3.7 COMPRESSION RATIO ON GENERATED TEXT\n",
    "        gen_texts = []\n",
    "        for _ in range(5):\n",
    "            prompt = \"The meaning of life is\"\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=80,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "            txt = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "            gen_texts.append(txt)\n",
    "\n",
    "        joined_gen = \"\\n\".join(gen_texts)\n",
    "        raw_bytes = joined_gen.encode(\"utf-8\")\n",
    "        if len(raw_bytes) > 0:\n",
    "            comp_bytes = gzip.compress(raw_bytes)\n",
    "            compression_ratio = len(comp_bytes) / len(raw_bytes)\n",
    "        else:\n",
    "            compression_ratio = float(\"nan\")\n",
    "\n",
    "        # 3.9 PACK RESULTS\n",
    "        return {\n",
    "            \"tokenizer\": name,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"params_millions\": n_params / 1e6,\n",
    "            \"tokens_per_char\": tokens_per_char,\n",
    "            \"val_loss\": avg_loss,\n",
    "            \"perplexity\": ppl,\n",
    "            \"bits_per_char\": bits_per_char,\n",
    "            }\n",
    "# 4. RUN EVALUATION FOR ALL TOKENIZERS & COLLECT RESULTS\n",
    "all_results = [evaluate_model(name) for name in tokenizer_names]\n",
    "\n",
    "results_df = pd.DataFrame(all_results).round(2)\n",
    "\n",
    "print(\"\\n=== Summary table ===\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98557ceb",
   "metadata": {},
   "source": [
    "## 95\\% conf interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e3e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QUICK GENERATION TESTS ===\n",
      "\n",
      "--- Generating with: gpt2 ---\n",
      "Hi my name is Jens, and I am from the village of the British Roman Empire . The first first was a common part of the first half of the Roman Church . The Church 's Church , the Church , the king 's name is the first part of the king 's Church , and\n",
      "[gpt2]\n",
      "  mean_tokens_per_sec:         72.13\n",
      "  tokens_per_sec_std:          2.50\n",
      "  tokens_per_sec_95%CI:        [71.42, 72.84]\n",
      "\n",
      "--- Generating with: google/byt5-small ---\n",
      "Hi my name is Jens, and I am from the second sincle of the Brit\n",
      "[google/byt5-small]\n",
      "  mean_tokens_per_sec:         104.61\n",
      "  tokens_per_sec_std:          5.36\n",
      "  tokens_per_sec_95%CI:        [103.08, 106.13]\n",
      "\n",
      "--- Generating with: unigram ---\n",
      "Hi m y name is Jens , and I am from a b r ' s on , as a c y , a ( ( : ) ) . A r a ) on s on , as a as a d e on ( ( ( ( F / / / / / / g / / / g\n",
      "[unigram]\n",
      "  mean_tokens_per_sec:         83.45\n",
      "  tokens_per_sec_std:          3.01\n",
      "  tokens_per_sec_95%CI:        [82.59, 84.31]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(\"\\n GENERATION TESTS\")\n",
    "\n",
    "prompt = \"Hi my name is Jens, and I am from\"\n",
    "n_runs = 50\n",
    "df = n_runs - 1\n",
    "t_95 = stats.t.ppf(1-0.05/2, df)\n",
    "\n",
    "for name in tokenizer_names:\n",
    "    print(f\"\\n Generating with: {name}\")\n",
    "    \n",
    "    model_dir = f\"model/{name}\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_dir).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # GENERATION TIME \n",
    "    tokens_per_sec_runs = []\n",
    "    gen_times = []\n",
    "    new_tokens_counts = []\n",
    "\n",
    "    sample_text = None  # only store the first output\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        with torch.no_grad():\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            t_gen0 = time.time()\n",
    "\n",
    "            generated = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,   # fixed for fairness\n",
    "                do_sample=True,\n",
    "                temperature=0.5,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            t_gen1 = time.time()\n",
    "\n",
    "        generation_time = t_gen1 - t_gen0\n",
    "\n",
    "        # count new tokens only\n",
    "        prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "        total_len = generated.shape[1]\n",
    "        n_gen_tokens = max(total_len - prompt_len, 0)\n",
    "\n",
    "        tokens_per_sec = n_gen_tokens / generation_time\n",
    "\n",
    "        tokens_per_sec_runs.append(tokens_per_sec)\n",
    "        gen_times.append(generation_time)\n",
    "        new_tokens_counts.append(n_gen_tokens)\n",
    "\n",
    "        if run == 0:  # save only the first generation\n",
    "            sample_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "    # MEAN + 95% CONFIDENCE INTERVAL\n",
    "    mean_tps = sum(tokens_per_sec_runs) / n_runs\n",
    "\n",
    "    var = sum((x - mean_tps) ** 2 for x in tokens_per_sec_runs) / (n_runs - 1)\n",
    "    std_tps = math.sqrt(var)\n",
    "    se_tps = std_tps / math.sqrt(n_runs)\n",
    "    ci_low = mean_tps - t_95 * se_tps\n",
    "    ci_high = mean_tps + t_95 * se_tps\n",
    "\n",
    "    mean_gen_time = sum(gen_times) / n_runs\n",
    "    mean_new_tokens = sum(new_tokens_counts) / n_runs\n",
    "\n",
    "    # OUTPUT\n",
    "    print(sample_text)  # ONE example per model\n",
    "\n",
    "    print(f\"[{name}]\")\n",
    "    print(f\"  mean_tokens_per_sec:         {mean_tps:.2f}\")\n",
    "    print(f\"  tokens_per_sec_std:          {std_tps:.2f}\")\n",
    "    print(f\"  tokens_per_sec_95%CI:        [{ci_low:.2f}, {ci_high:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fc0be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROMPT PROCESSING BENCHMARK (SIMPLIFIED) ===\n",
      "\n",
      " Prompt processing for: gpt2\n",
      "  total_time_mean:        0.074460 sec\n",
      "  total_time_std:         0.011028 sec\n",
      "  total_time_95%CI:       [0.071326, 0.077595]\n",
      "\n",
      " Prompt processing for: google/byt5-small\n",
      "  total_time_mean:        0.167579 sec\n",
      "  total_time_std:         0.045300 sec\n",
      "  total_time_95%CI:       [0.154705, 0.180453]\n",
      "\n",
      " Prompt processing for: unigram\n",
      "  total_time_mean:        0.081075 sec\n",
      "  total_time_std:         0.057978 sec\n",
      "  total_time_95%CI:       [0.064598, 0.097552]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n PROMPT PROCESSING BENCHMARK (SIMPLIFIED)\")\n",
    "\n",
    "long_prompt = (\n",
    "    \"\"\"This benchmark prompt is intentionally extended to approximately one thousand characters to evaluate how different tokenization strategies and model architectures process long, information-dense inputs under realistic conditions. It includes diverse elements such as technical terminology from machine learning, like stochastic gradient descent, entropy regularization, eigenvector drift, nonlinear optimization, and transformer attention patterns. It also integrates multilingual fragments such as hej, guten Tag, bonjour, hola, こんにちは, 你好, γειά σου, and привет to stress Unicode handling. Numerical values including 3.14159, 0.00027, and 42, alongside symbolic expressions like α→β→γ and ∫x² dx, test symbolic segmentation. Short code pieces such as for(i=0;i<10;i++){sum+=i;} and JSON-like fragments {\"key\":42,\"msg\":\"hello\"} further diversify the prompt, ensuring broad tokenizer coverage across subword granularity and vocabulary structure.\"\"\"\n",
    ")\n",
    "\n",
    "max_len_prompt = 1000\n",
    "n_runs = 50\n",
    "df = n_runs-1\n",
    "t_95 = stats.t.ppf(1-0.05/2, df)\n",
    "\n",
    "def mean_std_ci(values):\n",
    "    vals = [v for v in values if not math.isnan(v)]\n",
    "    m = sum(vals) / len(vals)\n",
    "    if len(vals) > 1:\n",
    "        var = sum((x - m)**2 for x in vals) / (len(vals) - 1)\n",
    "        s = math.sqrt(var)\n",
    "        se = s / math.sqrt(len(vals))\n",
    "        ci_low = m - t_95 * se\n",
    "        ci_high = m + t_95 * se\n",
    "    else:\n",
    "        s = float(\"nan\")\n",
    "        ci_low = ci_high = float(\"nan\")\n",
    "    return m, s, ci_low, ci_high\n",
    "\n",
    "for name in tokenizer_names:\n",
    "    print(f\"\\n Prompt processing for: {name}\")\n",
    "\n",
    "    model_dir = f\"model/{name}\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_dir).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_times = []\n",
    "    n_tokens = None\n",
    "\n",
    "    for run in range(n_runs):\n",
    "\n",
    "        # START total time\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Tokenization\n",
    "        encoded = tokenizer(\n",
    "            long_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_len_prompt,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "\n",
    "        if n_tokens is None:\n",
    "            n_tokens = encoded[\"input_ids\"].shape[1]\n",
    "\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "        # Forward pass (prompt only)\n",
    "        with torch.no_grad():\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            t_fwd0 = time.time()\n",
    "            _ = model(**encoded)\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            t_fwd1 = time.time()\n",
    "\n",
    "        # END total time\n",
    "        total_time = t_fwd1 - t0\n",
    "        total_times.append(total_time)\n",
    "\n",
    "    # STATISTICS\n",
    "    mean_t, std_t, ci_low, ci_high = mean_std_ci(total_times)\n",
    "\n",
    "    # REPORT\n",
    "    print(f\"  total_time_mean:        {mean_t:.6f} sec\")\n",
    "    print(f\"  total_time_std:         {std_t:.6f} sec\")\n",
    "    print(f\"  total_time_95%CI:       [{ci_low:.6f}, {ci_high:.6f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02465",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
